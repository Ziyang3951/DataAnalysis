# 主成分分析（Principal Component Analysis, PCA）

**主要思想即是降维**

## 1. 定义
主成分分析（PCA）是一种**无监督线性降维技术**，通过正交变换将原始高维数据映射到低维空间（主成分），使得新变量（主成分）能最大化保留数据的方差信息，并消除变量间的相关性。

## 2. 核心原理
### 2.1 数学基础
- ​**方差最大化**：寻找数据方差最大的投影方向作为第一主成分，后续主成分在正交约束下继续最大化剩余方差。
- ​**协方差矩阵**：数据标准化后计算协方差矩阵，反映变量间的线性相关性。
- ​**特征分解**：对协方差矩阵进行特征值分解，特征值大小对应主成分的方差贡献，特征向量确定投影方向。
- ​**正交性**：主成分之间彼此正交（不相关），形成新的坐标轴。

### 2.2 关键公式
- ​**协方差矩阵**：  
  $$\text{Cov}(X) = \frac{1}{n-1} X^T X$$
- ​**特征分解**：  
  $$\text{Cov}(X) \cdot \mathbf{v} = \lambda \mathbf{v}$$
  其中，
  $$\lambda$$
  为特征值，
  $$\mathbf{v}$$
  为特征向量。

## 3. 实现步骤
1. ​**数据标准化**​  
   将各变量标准化为均值为0、方差为1（避免量纲差异影响）。
2. ​**计算协方差矩阵**​  
   反映变量间的线性相关性。
3. ​**特征值分解**​  
   求解协方差矩阵的特征值和特征向量，按特征值降序排列。
4. ​**选择主成分数量**​  
   - ​**Kaiser准则**：保留特征值>1的主成分。
   - ​**累积方差贡献率**：通常选择累积贡献率≥85%的前k个主成分。
   - ​**碎石图（Scree Plot）​**：观察特征值下降拐点。
5. ​**数据投影**​  
   将原始数据投影到选定的主成分空间，得到低维表示：  
   $$Y = X \cdot W_k$$
   其中，
   $$W_k$$
   是由前k个特征向量组成的投影矩阵。

## 4. 应用场景
- ​**数据压缩**：减少特征维度，降低计算复杂度（如高分辨率图像处理）。
- ​**可视化**：将高维数据降至2D/3D，便于直观分析（如基因表达数据可视化）。
- ​**去噪与特征提取**：舍弃低方差成分（可能对应噪声），保留关键信息（如语音信号处理）。
- ​**消除共线性**：主成分正交，解决回归模型中的多重共线性问题。
- ​**金融分析**：构建股票收益率的主成分因子，简化投资组合优化。
- ​**模式识别**：经典案例包括人脸识别（Eigenfaces）中的特征降维。

## 5. 优缺点
### 5.1 优点
- ​**高效降维**：线性计算复杂度低，适合大规模数据。
- ​**去相关性**：主成分正交，消除原始变量间的冗余信息。
- ​**无监督性**：无需标签信息，适用于探索性数据分析。
- ​**可解释性**：主成分可表示为原始变量的线性组合，便于后续分析。

### 5.2 缺点
- ​**线性限制**：无法捕捉非线性关系（需结合核方法如Kernel PCA）。
- ​**方差≠信息**：方差小的成分可能包含重要分类信息（如类别边界特征）。
- ​**依赖标准化**：若未标准化，量纲差异会扭曲主成分方向。
- ​**异常值敏感**：方差最大化易受极端值影响，需提前处理异常值。

## 6. 代码示例（Python）
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA降维
pca = PCA(n_components=2)  # 保留2个主成分
X_pca = pca.fit_transform(X_scaled)

# 结果分析
print("方差贡献率:", pca.explained_variance_ratio_)
print("主成分方向:\n", pca.components_)
```
